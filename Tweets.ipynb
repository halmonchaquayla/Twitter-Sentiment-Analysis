{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:10.453106Z",
     "start_time": "2021-03-14T04:44:09.614941Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:10.501266Z",
     "start_time": "2021-03-14T04:44:10.455115Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/judge-1377884607_tweet_product_company.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:10.517320Z",
     "start_time": "2021-03-14T04:44:10.504278Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = df.drop('emotion_in_tweet_is_directed_at', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:10.581533Z",
     "start_time": "2021-03-14T04:44:10.519327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text            target\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  Negative emotion\n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  Positive emotion\n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...  Positive emotion\n",
       "3  @sxsw I hope this year's festival isn't as cra...  Negative emotion\n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  Positive emotion"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rename(columns={'is_there_an_emotion_directed_at_a_brand_or_product':'target'}, \n",
    "          inplace=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:12.276968Z",
     "start_time": "2021-03-14T04:44:10.583540Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import nltk\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:12.421447Z",
     "start_time": "2021-03-14T04:44:12.278975Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:12.437502Z",
     "start_time": "2021-03-14T04:44:12.423455Z"
    }
   },
   "outputs": [],
   "source": [
    "df1['tweet_text'] = df1.tweet_text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:12.533821Z",
     "start_time": "2021-03-14T04:44:12.442518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    .@wesley83 i have a 3g iphone. after 3 hrs twe...\n",
       "1    @jessedee know about @fludapp ? awesome ipad/i...\n",
       "2    @swonderlin can not wait for #ipad 2 also. the...\n",
       "3    @sxsw i hope this year's festival isn't as cra...\n",
       "4    @sxtxstate great stuff on fri #sxsw: marissa m...\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['clean_tweet'] = df1['tweet_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df1['clean_tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:15.406587Z",
     "start_time": "2021-03-14T04:44:15.043381Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_urls(corpus):\n",
    "#     pattern = re.compile(r'https?:\\/\\/\\S*')\n",
    "    pattern = re.compile(r\"(https?:\\/\\/)?(www\\.)[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,4}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)|(https?:\\/\\/)?(www\\.)?(?!ww)[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,4}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\") \n",
    "    # captures six different groups of websites\n",
    "    return pattern.sub(r'', corpus)\n",
    "\n",
    "df1['clean_tweet'] = df1['clean_tweet'].apply(lambda x:remove_urls(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:15.851370Z",
     "start_time": "2021-03-14T04:44:15.845350Z"
    }
   },
   "outputs": [],
   "source": [
    "# df1['tweet_text'] = df1['tweet_text'].str.replace(r\"({link})\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:16.667248Z",
     "start_time": "2021-03-14T04:44:16.286477Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_link(corpus):\n",
    "    pattern = re.compile(r\"({link})\")\n",
    "    return pattern.sub(r'', corpus)\n",
    "\n",
    "df1['clean_tweet'] = df1['clean_tweet'].apply(lambda x:remove_urls(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:17.381616Z",
     "start_time": "2021-03-14T04:44:17.301350Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_hashtags(corpus):\n",
    "    pattern = re.compile(r\"\\s?([@#][\\w_-]+)\")\n",
    "    return pattern.sub(r'', corpus)\n",
    "\n",
    "df1['clean_tweet'] = df1['clean_tweet'].apply(lambda x:remove_hashtags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:17.957524Z",
     "start_time": "2021-03-14T04:44:17.902342Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punct(corpus):\n",
    "    pattern = re.compile(r\"[\\|\\^\\+\\[\\]\\(\\),~\\'?\\.\\/{}=!$%&:;_-]\") # Match any character in set\n",
    "    return pattern.sub(r'', corpus)\n",
    "\n",
    "df1['clean_tweet'] = df1['clean_tweet'].apply(lambda x:remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:18.533938Z",
     "start_time": "2021-03-14T04:44:18.494305Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_numbers(corpus):\n",
    "    pattern = re.compile(r\"\\d\")\n",
    "    return pattern.sub(r'', corpus)\n",
    "\n",
    "df1['clean_tweet'] = df1['clean_tweet'].apply(lambda x: remove_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:20.337346Z",
     "start_time": "2021-03-14T04:44:20.326605Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# remove stopwords, tokenize and lemmatize the text\n",
    "# def preprocess(text, lemma=False):\n",
    "#     text = str(text).lower().strip()\n",
    "#     tokens = []\n",
    "#     for token in text.split():\n",
    "#         if token not in stopwords_list:\n",
    "#             if lemma:\n",
    "#                 tokens.append(lemmatizer.lemmatize(token))\n",
    "#             else:\n",
    "#                 tokens.append(token)\n",
    "#     return \" \".join(tokens)\n",
    "\n",
    "# df1['tweet_text']=df1['tweet_text'].apply(lambda x:preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:21.230522Z",
     "start_time": "2021-03-14T04:44:21.215474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:22.063281Z",
     "start_time": "2021-03-14T04:44:22.038198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          tweet_text  char_count\n",
      "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...         127\n",
      "1  @jessedee Know about @fludapp ? Awesome iPad/i...         139\n",
      "2  @swonderlin Can not wait for #iPad 2 also. The...          79\n",
      "3  @sxsw I hope this year's festival isn't as cra...          82\n",
      "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...         131\n",
      "104.95106125591114\n"
     ]
    }
   ],
   "source": [
    "df1['char_count'] = df1['tweet_text'].str.len() #how many characters do we have in description? \n",
    "print(df1[['tweet_text','char_count']].head())\n",
    "print(df1['char_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:24.915268Z",
     "start_time": "2021-03-14T04:44:24.439689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Counting down the days to #sxsw plus strong Ca...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  stopwords\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...          7\n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...          6\n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...          6\n",
       "3  @sxsw I hope this year's festival isn't as cra...          5\n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...          1\n",
       "5  @teachntech00 New iPad Apps For #SpeechTherapy...          0\n",
       "6                                                nan          0\n",
       "7  #SXSW is just starting, #CTIA is around the co...         14\n",
       "8  Beautifully smart and simple idea RT @madebyma...          4\n",
       "9  Counting down the days to #sxsw plus strong Ca...          5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['stopwords'] = df1['tweet_text'].apply(lambda x: len([x for x in x.split() if x in stopwords_list]))\n",
    "df1[['tweet_text','stopwords']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:44:26.426301Z",
     "start_time": "2021-03-14T04:44:26.030986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    g iphone hrs tweeting dead need upgrade plugin...\n",
       "1    know awesome ipadiphone app youll likely appre...\n",
       "2                                       wait also sale\n",
       "3     hope years festival isnt crashy years iphone app\n",
       "4    great stuff fri marissa mayer google tim oreil...\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "df1['clean_tweet'] = df1['clean_tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords_list))\n",
    "df1['clean_tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T03:35:42.899558Z",
     "start_time": "2021-03-14T03:35:39.234Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joined_tweet = ' '.join(df1['tweet_text'].tolist())\n",
    "joined_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T03:35:42.904575Z",
     "start_time": "2021-03-14T03:35:39.240Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_measure = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T03:35:42.906580Z",
     "start_time": "2021-03-14T03:35:39.242Z"
    }
   },
   "outputs": [],
   "source": [
    "tweet_finder = BigramCollocationFinder.from_words(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T03:35:42.901564Z",
     "start_time": "2021-03-14T03:35:39.236Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_tweet_bigram(corpus,n):\n",
    "#     vec=CountVectorizer(ngram_range=(2,2))\n",
    "#     bow=vec.fit_transform(corpus)\n",
    "#     sum_word=bow.sum(axis=0)\n",
    "#     word_freq=[(x,sum_word[0,i]) for x,i in vec.vocabulary_.items()]\n",
    "#     word_freq=sorted(word_freq,key=lambda x:x[1],reverse=True)\n",
    "#     return word_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T03:35:42.902568Z",
     "start_time": "2021-03-14T03:35:39.238Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "top_tweet_bigram=get_tweet_bigram(df1['tweet_text'],10)\n",
    "x,y=map(list,zip(*top_tweet_bigram))\n",
    "sns.barplot(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T03:35:42.908588Z",
     "start_time": "2021-03-14T03:35:39.246Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T03:35:42.909591Z",
     "start_time": "2021-03-14T03:35:39.248Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T03:35:42.911597Z",
     "start_time": "2021-03-14T03:35:39.250Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('googles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
